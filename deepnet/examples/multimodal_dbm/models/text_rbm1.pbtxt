name: "text_rbm1"
model_type: DBM
hyperparams {
  base_epsilon: 0.5
  epsilon_decay : INVERSE_T
  epsilon_decay_half_life : 100000
  initial_momentum : 0.0
  final_momentum : 0.0
  momentum_change_steps : 10000
  sparsity : true
  sparsity_target : 0.2
  sparsity_cost : 0.01
  sparsity_damping : 0.9
  apply_l2_decay: true
  l2_decay: 0.0001
  activation: LOGISTIC
  gibbs_steps: 1
  mf_steps: 1
  enable_display: false
}


layer {
  name: "text_input_layer"
  dimensions: 1
  numlabels: 2000
  is_input: true
  param {
    name: "bias"
    initialization: CONSTANT
  }
  data_field {
    train: "text_unlabelled"
    validation: "text_labelled"
  }
  hyperparams {
    activation: REPLICATED_SOFTMAX
    normalize: true
    normalize_to: 1
    dropout: false
    sparsity: false
  }
  loss_function: SQUARED_LOSS
  performance_stats {
    compute_error: true
  }
}

layer {
  name: "text_hidden1"
  dimensions: 1024
  shape:1024
  shape:1
  param {
    name: "bias"
    initialization: CONSTANT
  }
  performance_stats {
    compute_sparsity: true
  }
}

edge {
  node1: "text_input_layer"
  node2: "text_hidden1"
  directed: false
  up_factor: 2.0
  down_factor: 1.0
  param {
    name: "weight"
    initialization: DENSE_GAUSSIAN_SQRT_FAN_IN
    sigma : 1.0
  }
  hyperparams {
    enable_display: false
  }
}
